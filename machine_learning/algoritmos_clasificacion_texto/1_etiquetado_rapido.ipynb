{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0b172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c39194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('Here', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('enjoying', 'VBG'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"And now Here I am enjoying today\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63e30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "None\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets_json.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets_json')\n",
    "for tag in ['CC', 'RB', 'PRP', 'VBP', 'VBG', 'NN']:\n",
    "    print(nltk.help.upenn_tagset(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b536fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('permit', 'VB'),\n",
       " ('other', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('recidence', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras homonimas\n",
    "text = word_tokenize(\"they do not permit other people to get recidence permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48519f2f",
   "metadata": {},
   "source": [
    "## Etiquetado en Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c7364ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"cess_esp\")\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk import UnigramTagger as ut\n",
    "from nltk import BigramTagger as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9756ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_15896\\960797250.py:5: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  uni_tagger.evaluate(cess_sents[fraction:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5482623862354762"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etiquetado por unigramas\n",
    "cess_sents = cess.tagged_sents()\n",
    "fraction = int(len(cess_sents) * 0.9/100)\n",
    "uni_tagger = ut(cess_sents[:fraction])\n",
    "uni_tagger.evaluate(cess_sents[fraction:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44341469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yo', None),\n",
       " ('soy', None),\n",
       " ('una', 'di0fs0'),\n",
       " ('persona', None),\n",
       " ('muy', 'rg'),\n",
       " ('amable', None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_tagger.tag(word_tokenize(\"Yo soy una persona muy amable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3358c266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_15896\\1768576792.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  bi_tagger.evaluate(cess_sents[fraction+1:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1095272206303725"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento por bigramas\n",
    "fraction = int(len(cess_sents) * 90/100)\n",
    "bi_tagger = bt(cess_sents[:fraction])\n",
    "bi_tagger.evaluate(cess_sents[fraction+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eebb823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yo', 'pp1csn00'),\n",
       " ('soy', 'vsip1s0'),\n",
       " ('una', None),\n",
       " ('persona', None),\n",
       " ('muy', None),\n",
       " ('amable', None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_tagger.tag(word_tokenize(\"Yo soy una persona muy amable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a769d256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\david\\learning_platzi\\machine_learning\\algoritmos_clasificacion_texto\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 15.1MB/s]                    \n",
      "2026-01-02 17:51:49 INFO: Downloaded file to C:\\Users\\david\\stanza_resources\\resources.json\n",
      "2026-01-02 17:51:49 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.11.0/models/default.zip: 100%|██████████| 642M/642M [00:09<00:00, 64.6MB/s] \n",
      "2026-01-02 17:52:01 INFO: Downloaded file to C:\\Users\\david\\stanza_resources\\es\\default.zip\n",
      "2026-01-02 17:52:04 INFO: Finished downloading models and saved to C:\\Users\\david\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97103e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:52:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 8.80MB/s]                    \n",
      "2026-01-02 17:52:49 INFO: Downloaded file to C:\\Users\\david\\stanza_resources\\resources.json\n",
      "2026-01-02 17:52:49 WARNING: Language es package default expects mwt, which has been added\n",
      "2026-01-02 17:52:50 INFO: Loading these models for language: es (Spanish):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2026-01-02 17:52:50 INFO: Using device: cpu\n",
      "2026-01-02 17:52:50 INFO: Loading: tokenize\n",
      "2026-01-02 17:52:55 INFO: Loading: mwt\n",
      "2026-01-02 17:52:55 INFO: Loading: pos\n",
      "2026-01-02 17:52:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('es', processors=\"tokenize, pos\")\n",
    "doc = nlp('yo soy una persona muy amable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f67f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: yo\tpos: PRON\n",
      "word: soy\tpos: AUX\n",
      "word: una\tpos: DET\n",
      "word: persona\tpos: NOUN\n",
      "word: muy\tpos: ADV\n",
      "word: amable\tpos: ADJ\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f'word: {word.text}\\tpos: {word.pos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e32ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
